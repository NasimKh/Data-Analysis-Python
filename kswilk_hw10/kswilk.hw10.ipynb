{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats701 Homework 10, Winter 2018\n",
    "### Google Tensor Flow\n",
    "### Katherine Wilkinson\n",
    "#### kswilk@umich.edu\n",
    "\n",
    "I discussed this homework with Sam Edds and Roger and Spooner.\n",
    "\n",
    "Thanks for a great semester!! \n",
    ":) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Warmup: Constructing a 3-tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z = tf.zeros([5,4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.append([[0,0,1],[0,0,1],[0,0,1]],[[1,1,1]], axis = 0)\n",
    "x2= np.append([[0,0,0],[0,0,0],[0,0,1]],[[0,0,0]], axis = 0)\n",
    "x3= np.append([[0,0,0],[0,0,0],[0,1,1]],[[0,0,0]], axis = 0)\n",
    "x4= np.append([[0,0,0],[0,0,0],[0,0,1]],[[0,0,0]], axis = 0)\n",
    "x5 = np.append([[0,0,0],[0,0,0],[0,0,1]],[[0,0,0]], axis = 0)\n",
    "z = np.append([x1, x2],[x3,x4,x5], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflogo = tf.constant(z, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 1., 1.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tflogo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Building and training simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1: Logistic regression with a negative log-likelihood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "xtrain = np.load('logistic_xtrain.npy')\n",
    "ytrain = np.load('logistic_ytrain.npy')\n",
    "xtest = np.load('logistic_xtest.npy')\n",
    "ytest = np.load('logistic_ytest.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "##place holder for x and ytrue\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None,6])\n",
    "ytrue = tf.placeholder(dtype = tf.float32, shape = [None,1])\n",
    "\n",
    "##W and b as variables\n",
    "W = tf.Variable(tf.ones([6,1]))\n",
    "b = tf.Variable(tf.ones([1,1]))\n",
    "\n",
    "##Start session and initial all variables\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up sigmoid function and matrix multiplication of wx + b\n",
    "sig = tf.matmul(x,W) +b\n",
    "z = tf.sigmoid(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run session\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7534314\n"
     ]
    }
   ],
   "source": [
    "##Negative log likelihood funcion\n",
    "loss = tf.reduce_mean(-ytrue*sig + tf.log(1+tf.exp(sig)))\n",
    "\n",
    "##Find initial loss of xtrain and ytrain\n",
    "print(sess.run(loss, {x:xtrain, ytrue:ytrain}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.9214451 ],\n",
      "       [0.73044634],\n",
      "       [1.926627  ],\n",
      "       [2.533821  ],\n",
      "       [4.1213584 ],\n",
      "       [6.7668943 ]], dtype=float32), array([[-0.7739961]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess.run(init)\n",
    "\n",
    "##Use built in tf.train optimizer to find W and b\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    sess.run(train, {x: xtrain, ytrue: ytrain})\n",
    "print(sess.run([W,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33887318\n"
     ]
    }
   ],
   "source": [
    "## Find new loss on train data after optimization\n",
    "print(sess.run(loss, {x:xtrain, ytrue:ytrain}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Evaluating Logistic regression on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32646796\n"
     ]
    }
   ],
   "source": [
    "## Find negative log-likelihood on test data\n",
    "print(sess.run(loss, {x:xtest, ytrue:ytest}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Evaluating the estimated logistic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create new Variable for new optimized W and b\n",
    "#w_new = tf.Variable(sess.run(W), dtype = tf.float32)\n",
    "#b_new = tf.Variable(sess.run(b), dtype = tf.float32)\n",
    "#sig_new = tf.matmul(x,w_new) +b_new\n",
    "#z = tf.sigmoid(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "##input actual W and b values\n",
    "w_act = tf.constant([[1],[1],[2],[3],[5],[8]], dtype = tf.float32)\n",
    "b_act = tf.constant([-1], dtype = tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.645175]], dtype=float32)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find square error\n",
    "sq_err = tf.square(W - w_act) \n",
    "tot_err = tf.reduce_sum(sq_err) + tf.square(b - b_act)\n",
    "sess.run(tot_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Classification of normally distributed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data and start session\n",
    "sess = tf.Session()\n",
    "xtrain_n = np.load('normal_xtrain.npy')\n",
    "ytrain_n = np.load('normal_ytrain.npy')\n",
    "xtest_n = np.load('normal_xtest.npy')\n",
    "ytest_n = np.load('normal_ytest.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##place holder for x and ytrue\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None,1])\n",
    "ytrue = tf.placeholder(dtype = tf.float32, shape = [None,3])\n",
    "\n",
    "##Variable for mu and standard deviation\n",
    "mu = tf.Variable(tf.ones([1,3]))\n",
    "s = tf.Variable(tf.ones([1,3]))\n",
    "\n",
    "##Calculate normal distribution probabilites at each x\n",
    "pdf = tf.distributions.Normal(loc = mu, scale = s)\n",
    "p = pdf.prob(x)\n",
    "\n",
    "##Calculate cross entropy by summing over the sum of all the pdfs\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(-ytrue*tf.log(p),reduction_indices = [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.9810063 ,  0.00807612,  3.0174103 ]], dtype=float32), array([[0.5114226, 1.0042008, 1.4121261]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "##Optimize with Adagrad Optimizer\n",
    "train_step = tf.train.AdagradOptimizer(0.05).minimize(cross_entropy)\n",
    "\n",
    "##Intiate all globale variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "##Add placeholders for new x and new p for work in Google Cloud\n",
    "x_new = tf.placeholder(dtype= tf.float32, shape = [None,1])\n",
    "p_new = pdf.prob(x_new)\n",
    "predval = tf.argmax(p_new,1)\n",
    "\n",
    "##Run over 10,000 iterations to optimize\n",
    "for i in range(10000):\n",
    "    sess.run(train_step, {x: xtrain_n, ytrue: ytrain_n})\n",
    "##print new optimized means and variances\n",
    "print(sess.run([mu,s**2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Evaluating loss on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3530648\n"
     ]
    }
   ],
   "source": [
    "## find cross-entropy of model on test data\n",
    "print(sess.run(cross_entropy, {x:xtest_n, ytrue:ytest_n}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Evaluating Parameter estimation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set actual mean and variance values\n",
    "mu_act = tf.constant([-1.0, 0.0, 3.0], dtype = tf.float32)\n",
    "s2_act = tf.constant([0.5, 1.0, 1.5], dtype = tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008599052"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Calculate square error\n",
    "sq_err = tf.square(mu - mu_act) + tf.square(s**2 - s2_act)\n",
    "tot_err = tf.reduce_sum(sq_err)\n",
    "sess.run(tot_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Evaluating classification error on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "##Find classification error of estimated model\n",
    "cor_pred = tf.equal(tf.argmax(p, 1), tf.argmax(ytrue,1))\n",
    "acc = tf.reduce_mean(tf.cast(cor_pred, tf.float32))\n",
    "print(sess.run(acc, feed_dict = {x:xtest_n, ytrue:ytest_n}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Running Models on Google Cloud Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.saved_model.simple_save(sess, \"./kswilk_normal_trained\", inputs = {'x':x_new}, outputs = {'z':predval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gcloud ml-engine versions describe \"kswilk_hw10\" --model \"kswilk_stat701_hw10_normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-356-e3cd47e27bb7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-356-e3cd47e27bb7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    kswilk@cloudshell:~$ emacs kswilk.instance.json\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "kswilk@cloudshell:~$ emacs kswilk.instance.json\n",
    "kswilk@cloudshell:~$ MODEL_NAME=\"kswilk_stat701_hw10_normal\"\n",
    "kswilk@cloudshell:~$ INPUT_DATA_FILE=\"kswilk.instance.json\"\n",
    "kswilk@cloudshell:~$ VERSION_NAME=\"kswilk_hw10\"\n",
    "kswilk@cloudshell:~$ gcloud ml-engine predict --model $MODEL_NAME  \\\n",
    ">                    --version $VERSION_NAME \\\n",
    ">                    --json-instances $INPUT_DATA_FILE\n",
    "Z\n",
    "2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
